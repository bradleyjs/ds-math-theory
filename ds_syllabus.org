#+TITLE: Syllabus for Math and Theory of Data Science
#+OPTIONS: ^:nil num:nil

* Overview

** About Exercises
*** Proofs
    1. If a proof is listed as an exercise, it means you should learn the
       proof so you can reproduce it and explain it to somebody else
       without notes.
       1. An excessive focus on perfection is, of course, detrimental. That
          said, this level of understanding, particularly the "without
          notes" part, is /always/ the goal.
       2. If you can't do this, you don't yet fully understand the
          proof.
       3. If we list a proof as an exercise, it is because we feel that
          understanding it helps you to understand an important or
          fundamental idea.


* Introductory Resources

** Mathematical background
   1. [[https://math.berkeley.edu/~gbergman/ug.hndts/sets_etc,t=1.pdf]["Some notes on sets, logic, and mathematical language"]]
   2. "Reading Mathematics" in http://matrixeditions.com/VC5.Chap0.pdf
      (from /Vector Calculus, Linear Algebra, and Differential Forms/)

** Linear Algebra
   1. [[http://joshua.smcvt.edu/linearalgebra/][/Linear Algebra/]] (Hefferon)
      - Complete text available free online. Recommended as supplement to
        /Linear Algebra and Learning from Data/.
   2. [[http://matrixeditions.com/5thUnifiedApproach.html][/Vector Calculus, Linear Algebra, and Differential Forms/]] (Hubbard).  This is
      an excellent book and I recommend it highly, but it's focus is very
      much on pure mathematics. It has many beautiful proofs.
      1. I currently have the 3rd edition, which can be found used pretty
         inexpensively. If you are using these notes, try to get the 2nd
         edition or newer.
   3. [[http://vmls-book.stanford.edu/][/Introduction to Applied Linear Algebra: Vectors, matrices,and least squares/]] (Boyd)
      - Complete text available free online.


* Meeting 1: The Functions of Deep Learning

** Materials
  1. [[http://math.mit.edu/~gs/learningfromdata/siam.pdf]["The Functions of Deep Learning"]]
  2. The introductory material from /Linear Algebra and Learning from Data/ (/LALD/):
     1. http://math.mit.edu/~gs/learningfromdata/dsla_dlnn.pdf, and
     2. http://math.mit.edu/~gs/learningfromdata/dsla_preface.pdf
  3. Two videos from Strang's linear algebra course:
     1. [[https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/ax-b-and-the-four-subspaces/the-geometry-of-linear-equations/]["The Geometry of Linear Equations"]], 
     2. [[https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/ax-b-and-the-four-subspaces/an-overview-of-key-ideas/]["An Overview of Key Ideas"]] 
  4. /LALD/ I. 1, [[http://math.mit.edu/~gs/learningfromdata/dsla1-1.pdf]["Multiplication of Ax using Columns of A"]]

*** Comments
  - You can also find Strang's lectures for MIT 18.06 on YouTube:
    https://youtu.be/ZK3O402wf1c. Also good, covering basically the same
    ideas in the "Overview of Key Ideas" lecture above:
    https://www.youtube.com/watch?v=ggWYkes-n6E
  - We will link to the OCW materials for MIT 18.065 as they become
    available.

** Suggested exercises
   1. Understand the proof of the rank theorem ("In every matrix, the
      number of independent columns is equal to the number of independent
      rows") on page 5.
   2. Suggested exercises from Problem Set I.1: 1-3, 6, 12, 19; 20*, 21*,
      22*.
